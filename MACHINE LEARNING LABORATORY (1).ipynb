{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fff648",
   "metadata": {},
   "source": [
    "# 1)a)demonstrate the FIND-S algorithm for finding the most specifichypothesis based on a given set of training data samples. Read the training data from a.CSV file and show the output for test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1bb7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 'Sunny' 'Warm' 'Normal' 'Strong' 'Warm' '?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('finds.csv')\n",
    "def train(concepts, target):\n",
    "    for i,val in enumerate(target):\n",
    "        if val == \"Yes\":\n",
    "            specific_h = concepts[i]\n",
    "        break\n",
    "\n",
    "    for i,h in enumerate(concepts):\n",
    "        if target[i] == \"Yes\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] == specific_h[x]:\n",
    "                    pass\n",
    "            else:\n",
    "                specific_h[x] = \"?\"\n",
    "                return specific_h\n",
    "#slicing rows and column, : means begining to end of row\n",
    "concepts = np.array(data.iloc[:,0:-1])\n",
    "target = np.array(data.iloc[:,-1])\n",
    "print(train(concepts,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97161419",
   "metadata": {},
   "source": [
    "# 1)b)Develop an interactive program byCompareing the result by implementing LIST THEN ELIMINATE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5f4080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific hypothesis obtained by LIST THEN ELIMINATE algorithm:\n",
      "['?' 'Sunny' 'Warm' 'Normal' 'Strong' 'Warm' 'Same']\n",
      "\n",
      "Specific hypothesis obtained by the original code:\n",
      "[1 'Sunny' 'Warm' 'Normal' 'Strong' 'Warm' '?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('finds.csv')\n",
    "\n",
    "def list_then_eliminate(concepts, target):\n",
    "    positive_examples = concepts[target == 'Yes']\n",
    "    specific_hypothesis = positive_examples[0].copy()\n",
    "\n",
    "    for example in positive_examples[1:]:\n",
    "        for i, attribute in enumerate(example):\n",
    "            if attribute != specific_hypothesis[i]:\n",
    "                specific_hypothesis[i] = '?'\n",
    "\n",
    "                return specific_hypothesis\n",
    "\n",
    "concepts = np.array(data.iloc[:,0:-1])\n",
    "target = np.array(data.iloc[:,-1])\n",
    "\n",
    "print('Specific hypothesis obtained by LIST THEN ELIMINATE algorithm:')\n",
    "print(list_then_eliminate(concepts, target))\n",
    "print('\\nSpecific hypothesis obtained by the original code:')\n",
    "print(train(concepts, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90906f42",
   "metadata": {},
   "source": [
    "# 2)For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm. Output a description of the set of all hypotheses consistent with the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c19fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final S:\n",
      "[1 'Sunny' 'Warm' 'Normal' 'Strong' 'Warm' 'Same']\n",
      "Final G:\n",
      "[['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?', '?']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eg</th>\n",
       "      <th>Sky</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Water</th>\n",
       "      <th>Forecast</th>\n",
       "      <th>Enjoyable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Warm</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cold</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Warm</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Warm</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Eg    Sky Temperature Humidity    Wind Water Forecast Enjoyable\n",
       "0   1  Sunny        Warm   Normal  Strong  Warm     Same       Yes\n",
       "1   2  Sunny        Warm     High  Strong  Warm     Same       Yes\n",
       "2   3  Rainy        Cold     High  Strong  Warm     Same       Yes\n",
       "3   4  Sunny        Warm     High  Strong  Warm     Same       Yes"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Loading Data from CSV file\n",
    "data=pd.DataFrame(data=pd.read_csv(\"finds.csv\"))\n",
    "#Seperating concept features from Target\n",
    "concepts=np.array(data.iloc[:,0:-1])\n",
    "#Isolating target into seperate Dataframe\n",
    "target=np.array(data.iloc[:,-1])\n",
    "def learn(concepts,target):\n",
    "    specific_h = concepts[0].copy()\n",
    "    general_h=[[\"?\"for i in range(len(specific_h))]for i in range (len(specific_h))]\n",
    "    #the learning iterations\n",
    "    for i, h in enumerate(concepts):\n",
    "        #checking if the hypothesys has a positive target\n",
    "        if target[i]==\"Yes\":\n",
    "            for x in range(len(specific_h)):\n",
    "            #Change values in S&G only if values change\n",
    "                if h[x]!=specific_h[x]:\n",
    "                    specific_h[x]='?'\n",
    "                    general_h[x][x]='?'\n",
    "                #Checking if the hypothesys has a positive target\n",
    "                if target[i]==\"No\":\n",
    "                    for x in range (len(specific_h)):\n",
    "                        #for negative hypothesys change the value only in G\n",
    "                        if h[x]!=specific_h[x]:\n",
    "                            general_h[x][x]='?'\n",
    "                            #find indices where we have empty row, meaning those that are unchanged\n",
    "                            indices=[i for i ,val in enumerate(general_h)if val==['?','?','?','?','?','?']]\n",
    "                            for i in indices:\n",
    "                                #remove those rows form general_h\n",
    "                                general_h.remove(['?','?','?','?','?','?'])\n",
    "                                #return final values\n",
    "        return specific_h,general_h\n",
    "\n",
    "                        \n",
    "s_final, g_final=learn(concepts,target)                    \n",
    "print(\"Final S:\",s_final,sep=\"\\n\")\n",
    "print(\"Final G:\",g_final,sep=\"\\n\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5dda9c",
   "metadata": {},
   "source": [
    "# 3)Demonstrate Pre processing (Data Cleaning, Integration and Transformation) activity on suitable data: For example: Identify and Delete Rows that Contain Duplicate Data by considering an appropriate dataset.    Identify and Delete Columns That Contain a Single Value by considering an appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba0ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing duplicates in dataset 1:\n",
      "   A  B\n",
      "0  1  5\n",
      "1  2  6\n",
      "2  2  6\n",
      "3  3  7\n",
      "4  4  8\n",
      "5  5  9\n",
      "6  5  9\n",
      "\n",
      "After removing duplicates in dataset 1:\n",
      "   A  B\n",
      "0  1  5\n",
      "1  2  6\n",
      "3  3  7\n",
      "4  4  8\n",
      "5  5  9\n",
      "\n",
      "Before removing single value columns in dataset 2:\n",
      "    C   D\n",
      "0  10  11\n",
      "1  10  12\n",
      "2  10  13\n",
      "3  10  14\n",
      "4  10  15\n",
      "\n",
      "After removing single value columns in dataset 2:\n",
      "    D\n",
      "0  11\n",
      "1  12\n",
      "2  13\n",
      "3  14\n",
      "4  15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset 1: contains duplicate rows\n",
    "data1 = {'A': [1, 2, 2, 3, 4, 5, 5],\n",
    "         'B': [5, 6, 6, 7, 8, 9, 9]}\n",
    "\n",
    "# Sample dataset 2: contains a column with a single value\n",
    "data2 = {'C': [10, 10, 10, 10, 10],\n",
    "         'D': [11, 12, 13, 14, 15]}\n",
    "\n",
    "# Creating dataframes from the datasets\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Identifying and deleting duplicate rows in df1\n",
    "print(\"Before removing duplicates in dataset 1:\")\n",
    "print(df1)\n",
    "df1.drop_duplicates(inplace=True)\n",
    "print(\"\\nAfter removing duplicates in dataset 1:\")\n",
    "print(df1)\n",
    "\n",
    "# Identifying and deleting columns with a single value in df2\n",
    "print(\"\\nBefore removing single value columns in dataset 2:\")\n",
    "print(df2)\n",
    "cols_to_remove = [col for col in df2.columns if df2[col].nunique() <= 1]\n",
    "df2.drop(cols_to_remove, axis=1, inplace=True)\n",
    "print(\"\\nAfter removing single value columns in dataset 2:\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f5262",
   "metadata": {},
   "source": [
    "# 4)Demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge toclassify a new  sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2147a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 15\n",
      "{'outlook': {'id': 'wind', '1': 'weak', '2': 'strong', '3': 'weak', '4': 'weak', '5': 'weak', '6': 'weak', '7': 'strong', '8': 'weak', '9': 'weak', '10': 'weak', '11': 'strong', '12': 'strong', '13': 'weak', '14rainy': 'no'}}\n",
      "Null\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "#Function tells which class has more entries in given data-set\n",
    "def majorClass(attributes, data, target):\n",
    "    freq = {}\n",
    "    index = attributes.index(target)\n",
    "    for tuple in data:\n",
    "        if tuple[index] in freq:\n",
    "            freq[tuple[index]] += 1\n",
    "        else:\n",
    "            freq[tuple[index]] = 1\n",
    "    max = 0\n",
    "    major = \"\"\n",
    "    for key in freq.keys():\n",
    "        if freq[key]>max:\n",
    "            max = freq[key]\n",
    "            major = key\n",
    "    return major\n",
    "\n",
    "# Calculates the entropy of the data given the target attribute\n",
    "def entropy(attributes, data, targetAttr):\n",
    "    freq = {}\n",
    "    dataEntropy = 0.0\n",
    "    i = 0\n",
    "    for entry in attributes:\n",
    "        if (targetAttr == entry):\n",
    "            break\n",
    "        i = i + 1\n",
    "\n",
    "    i = i - 1\n",
    "    for entry in data:\n",
    "        if entry[i] in freq:\n",
    "            freq[entry[i]] += 1.0\n",
    "        else:\n",
    "            freq[entry[i]] = 1.0\n",
    "    for freq in freq.values():\n",
    "        dataEntropy += (-freq/len(data)) * math.log(freq/len(data), 2)\n",
    "\n",
    "    return dataEntropy\n",
    "\n",
    "# Calculates the information gain (reduction in entropy) in the data when a particular attribute is chosen for splitting the data.\n",
    "def info_gain(attributes, data, attr, targetAttr):\n",
    "    freq = {}\n",
    "    subsetEntropy = 0.0\n",
    "    i = attributes.index(attr)\n",
    "    for entry in data:\n",
    "        if entry[i] in freq:\n",
    "            freq[entry[i]] += 1.0\n",
    "        else:\n",
    "            freq[entry[i]] = 1.0\n",
    "\n",
    "    for val in freq.keys():\n",
    "        valProb = freq[val] / sum(freq.values())\n",
    "        dataSubset = [entry for entry in data if entry[i] == val]\n",
    "        subsetEntropy += valProb * entropy(attributes, dataSubset, targetAttr)\n",
    "\n",
    "    return (entropy(attributes, data, targetAttr) - subsetEntropy)\n",
    "\n",
    "# This function chooses the attribute among the remaining attributes which has the maximum information gain.\n",
    "def attr_choose(data, attributes, target):\n",
    "    best = attributes[0]\n",
    "    maxGain = 0;\n",
    "    for attr in attributes:\n",
    "        newGain = info_gain(attributes, data, attr, target)\n",
    "        if newGain>maxGain:\n",
    "            maxGain = newGain\n",
    "            best = attr\n",
    "\n",
    "    return best\n",
    "\n",
    "# This function will get unique values for that particular attribute from the given data\n",
    "def get_values(data, attributes, attr):\n",
    "    index = attributes.index(attr)\n",
    "    values = []\n",
    "    for entry in data:\n",
    "        if entry[index] not in values:\n",
    "            values.append(entry[index])\n",
    "\n",
    "    return values\n",
    "\n",
    "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
    "def get_data(data, attributes, best, val):\n",
    "    new_data = [[]]\n",
    "    index = attributes.index(best)\n",
    "    for entry in data:\n",
    "        if (entry[index] == val):\n",
    "            newEntry = []\n",
    "            for i in range(0,len(entry)):\n",
    "                if(i != index):\n",
    "                    newEntry.append(entry[i])\n",
    "            new_data.append(newEntry)\n",
    "\n",
    "    new_data.remove([])\n",
    "    return new_data\n",
    "\n",
    "def build_tree(data, attributes, target):\n",
    "    data = data[:]\n",
    "    vals = [record[attributes.index(target)] for record in data]\n",
    "    default = majorClass(attributes, data, target)\n",
    "    if not data or (len(attributes) - 1) <= 0:\n",
    "        return default\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        best = attr_choose(data, attributes, target)\n",
    "        tree = {best: {}}\n",
    "        for val in get_values(data, attributes, best):\n",
    "            new_data = get_data(data, attributes, best, val)\n",
    "            newAttr = attributes[:]\n",
    "            newAttr.remove(best)\n",
    "            subtree = build_tree(new_data, newAttr, target)\n",
    "            tree[best][val] = subtree\n",
    "        return tree\n",
    "\n",
    "\n",
    "def execute_decision_tree():\n",
    "    data = []\n",
    "    #load file\n",
    "    with open(\"weather.csv\") as tsv:\n",
    "        for line in csv.reader(tsv):\n",
    "            data.append(tuple(line))\n",
    "    print(\"Number of records:\", len(data))\n",
    "\n",
    "    #set attributes\n",
    "    attributes = ['outlook', 'temperature', 'humidity', 'wind', 'play']\n",
    "    target = attributes[-1]\n",
    "\n",
    "    #set training data\n",
    "    acc = []\n",
    "    training_set = [x for i, x in enumerate(data)]\n",
    "    tree = build_tree(training_set, attributes, target)\n",
    "    print(tree)\n",
    "\n",
    "    #execute algorithm on test data\n",
    "    results = []\n",
    "    test_set = [('rainy', 'mild', 'high', 'strong')]\n",
    "    for entry in test_set:\n",
    "        tempDict = tree.copy()\n",
    "        result = \"\"\n",
    "        while isinstance(tempDict, dict):\n",
    "            child = []\n",
    "            nodeVal = next(iter(tempDict))\n",
    "            child = tempDict[next(iter(tempDict))].keys()\n",
    "            tempDict = tempDict[next(iter(tempDict))]\n",
    "            index = attributes.index(nodeVal)\n",
    "            value = entry[index]\n",
    "\n",
    "            if value in tempDict.keys():\n",
    "                result = tempDict[value]\n",
    "                tempDict = tempDict[value]\n",
    "            else:\n",
    "                result = \"Null\"\n",
    "                break\n",
    "\n",
    "        if result != \"Null\":\n",
    "            results.append(result == entry[-1])\n",
    "\n",
    "        print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    execute_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846ec8e",
   "metadata": {},
   "source": [
    "# 5) Demonstrate the working of the Random forest algorithm. Use an appropriate data set for building and apply this knowledge toclassify a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4855db7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting random-forest-mcNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading random_forest_mc-1.0.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from random-forest-mc) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.60 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from random-forest-mc) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from random-forest-mc) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from pandas>=1.3->random-forest-mc) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from pandas>=1.3->random-forest-mc) (2022.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\hi\\anaconda3\\lib\\site-packages (from tqdm>=4.60->random-forest-mc) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.3->random-forest-mc) (1.16.0)\n",
      "Installing collected packages: random-forest-mc\n",
      "Successfully installed random-forest-mc-1.0.3\n"
     ]
    }
   ],
   "source": [
    "pip install random-forest-mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3c7240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Predicted class of new sample: [2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load irs dataset\n",
    "iris = load_iris()\n",
    "# Create feature and target amays\n",
    "X=iris.data\n",
    "\n",
    "y = iris.target\n",
    "\n",
    "#Splitinto training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the model using the training sets y_pred=clf predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Perform prediction on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Check the accuracy of the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classify anew sample\n",
    "new_sample = [(3, 5, 4, 2)]\n",
    "\n",
    "new_pred = clf.predict(new_sample)\n",
    "print(\"Predicted class of new sample:\" , new_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40749438",
   "metadata": {},
   "source": [
    "# 6)Implement the naïve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1af3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# load the training dataset from the .CSV file\n",
    "data = pd.read_csv(\"finds.csv\")\n",
    "\n",
    "\n",
    "# split the data into features and target variables\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "\n",
    "# encode categorical variables if necessary\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# create a Naive Bayes classifier object\n",
    "nb = GaussianNB()\n",
    "\n",
    "# train the classifier on the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "# evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede2281",
   "metadata": {},
   "source": [
    "# 7)Assuming a set of documents that need to be classified, use the naive Bayesian Classifier model to perform this task. Calculate the accuracy, precision, and recall for your data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82add759",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m newsgroups_train \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m newsgroups_test \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# vectorize the documents\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:269\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    268\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     cache \u001b[38;5;241m=\u001b[39m \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20Newsgroups dataset not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:83\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     78\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(archive_path)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[0;32m     81\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     82\u001b[0m     train\u001b[38;5;241m=\u001b[39mload_files(train_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m---> 83\u001b[0m     test\u001b[38;5;241m=\u001b[39m\u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m compressed_content \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mencode(pickle\u001b[38;5;241m.\u001b[39mdumps(cache), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzlib_codec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_base.py:257\u001b[0m, in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[0m\n\u001b[0;32m    255\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m--> 257\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     data \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mdecode(encoding, decode_error) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pathlib.py:1126\u001b[0m, in \u001b[0;36mPath.read_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "# vectorize the documents\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# target labels\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# create a Naive Bayes Classifier model\n",
    "nb_classifier = MultinomialNB()\n",
    "# train the model on the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# calculate the accuracy, precision, and recall of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6001d99",
   "metadata": {},
   "source": [
    "# 8)Construct aBayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67a53f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2633935853.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    xray = Mixture(tuberculosis, Mixture, lung, Categorical, or ([0.96, 0.04], [0.115, 0.885]))\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bayespy.nodes import Categorical,Mixture\n",
    "from bayespy.inference import VB\n",
    "import numpy as np\n",
    "\n",
    "return np.take([p_false, p_true], [[FALSE, TRUE], [TRUE, TRUE]], axis=0)\n",
    "\n",
    "asia = Categorical([0.5, 0.5])\n",
    "tuberculosis = Mixture(asia, Categorical, [[0.99, 0.01], [0.8, 0.2]])\n",
    "smoking = Categorical([0.5, 0.5])\n",
    "lung = Mixture(smoking, Categorical, [[0.98, 0.02], [0.25, 0.75]])\n",
    "bronchitis = Mixture(smoking, Categorical, [[0.97, 0.03], [0.08, 0.92]])\n",
    "xray = Mixture(tuberculosis, Mixture, lung, Categorical, or ([0.96, 0.04], [0.115, 0.885]))\n",
    "\n",
    "dyspnea = Mixture(bronchitis, Mixture, tuberculosis, Mixture, lung, Categorical, or([0.6,0.4],[0.18,0.82]), or([0.11,0.89],[0.04,0.96]))\n",
    "tuberculosis.observe(TRUE)\n",
    "smoking.observe(FALSE)\n",
    "bronchitis.observe(TRUE)\n",
    "Q=VB(dyspnea,xray,bronchitis,lung,smoking,tuberculosis,asia)\n",
    "Q.update(repeat=100)\n",
    "print(\"P(asia):\",asia.get_moments()[0][TRUE])\n",
    "print(\"P(tuberculosis):\",tuberculosis.get_moments()[0][TRUE])\n",
    "print(\"P(smoking):\",smoking.get_moments()[0][TRUE])\n",
    "print(\"P(lung):\",lung.get_moments()[0][TRUE])\n",
    "print(\"P(bronchitis):\",bronchitis.get_moments()[0][TRUE])\n",
    "print(\"P(xray):\",xray.get_moments()[0][TRUE])\n",
    "print(\"P(dyspnea):\",dyspnea.get_moments()[0][TRUE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa314c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayespy\n",
      "  Downloading bayespy-0.5.26.tar.gz (401 kB)\n",
      "     -------------------------------------- 401.7/401.7 kB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.10.0 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from bayespy) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.13.0 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from bayespy) (1.10.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\hi\\anaconda3\\lib\\site-packages (from bayespy) (3.7.0)\n",
      "Building wheels for collected packages: bayespy\n",
      "  Building wheel for bayespy (setup.py): started\n",
      "  Building wheel for bayespy (setup.py): finished with status 'done'\n",
      "  Created wheel for bayespy: filename=bayespy-0.5.26-py3-none-any.whl size=377662 sha256=3c9333cea1e02437233cf5a9682a9969794700f9a0970922ea71713dccd0bba4\n",
      "  Stored in directory: c:\\users\\hi\\appdata\\local\\pip\\cache\\wheels\\17\\b2\\f9\\55d4f52e600e891f31b66b776199e078c4a94de2c127ab3c4a\n",
      "Successfully built bayespy\n",
      "Installing collected packages: bayespy\n",
      "Successfully installed bayespy-0.5.26\n"
     ]
    }
   ],
   "source": [
    "!pip install bayespy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c2861a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The mixed distribution does not have a plates axis for the cluster plate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m bronchitis \u001b[38;5;241m=\u001b[39m Mixture(smoking, Categorical, [np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.97\u001b[39m, \u001b[38;5;241m0.03\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.92\u001b[39m])])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#xray = Mixture(tuberculosis, Mixture, lung, Categorical, np.array([0.96, 0.04]), np.array([0.115, 0.885]))\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m xray \u001b[38;5;241m=\u001b[39m Mixture(tuberculosis, \u001b[43mMixture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlung\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, Categorical, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.115\u001b[39m, \u001b[38;5;241m0.885\u001b[39m]))\n\u001b[0;32m     20\u001b[0m dyspnea \u001b[38;5;241m=\u001b[39m Mixture(bronchitis, Mixture, tuberculosis, Mixture, lung, Categorical, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.4\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.18\u001b[39m, \u001b[38;5;241m0.82\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.11\u001b[39m, \u001b[38;5;241m0.89\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.04\u001b[39m, \u001b[38;5;241m0.96\u001b[39m]))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Observations\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayespy\\inference\\vmp\\nodes\\mixture.py:431\u001b[0m, in \u001b[0;36mMixture.__init__\u001b[1;34m(self, z, node_class, cluster_plate, *params, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, node_class, \u001b[38;5;241m*\u001b[39mparams, cluster_plate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_plate \u001b[38;5;241m=\u001b[39m cluster_plate\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(z, node_class, \u001b[38;5;241m*\u001b[39mparams, cluster_plate\u001b[38;5;241m=\u001b[39mcluster_plate,\n\u001b[0;32m    432\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayespy\\inference\\vmp\\nodes\\expfamily.py:82\u001b[0m, in \u001b[0;36museconstructor.<locals>.constructor_decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstructor_decorator\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_moments \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_moments \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     81\u001b[0m         (args, kwargs, dims, plates, dist, stats, pstats) \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 82\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution \u001b[38;5;241m=\u001b[39m dist\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayespy\\inference\\vmp\\nodes\\mixture.py:449\u001b[0m, in \u001b[0;36mMixture._constructor\u001b[1;34m(cls, z, node_class, cluster_plate, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# Check that at least one of the parents has the cluster plate axis\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mixture_plates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mabs\u001b[39m(cluster_plate):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe mixed distribution does not have a plates \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis for the cluster plate axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Resolve the number of clusters\u001b[39;00m\n\u001b[0;32m    453\u001b[0m mixture_plates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mixture_plates)\n",
      "\u001b[1;31mValueError\u001b[0m: The mixed distribution does not have a plates axis for the cluster plate axis"
     ]
    }
   ],
   "source": [
    "from bayespy.nodes import Categorical, Mixture\n",
    "from bayespy.inference import VB\n",
    "import numpy as np\n",
    "\n",
    "TRUE = 1\n",
    "FALSE = 0\n",
    "\n",
    "p_false = 0.0  # Replace with the actual probability values\n",
    "p_true = 1.0  # Replace with the actual probability values\n",
    "\n",
    "# Define the conditional probability tables\n",
    "asia = Categorical([0.5, 0.5])\n",
    "tuberculosis = Mixture(asia, Categorical, [np.array([0.99, 0.01]), np.array([0.8, 0.2])])\n",
    "smoking = Categorical([0.5, 0.5])\n",
    "lung = Mixture(smoking, Categorical, [np.array([0.98, 0.02]), np.array([0.25, 0.75])])\n",
    "bronchitis = Mixture(smoking, Categorical, [np.array([0.97, 0.03]), np.array([0.08, 0.92])])\n",
    "#xray = Mixture(tuberculosis, Mixture, lung, Categorical, np.array([0.96, 0.04]), np.array([0.115, 0.885]))\n",
    "xray = Mixture(tuberculosis, Mixture(lung, Categorical, np.array([0.96, 0.04])), Categorical, np.array([0.115, 0.885]))\n",
    "\n",
    "dyspnea = Mixture(bronchitis, Mixture, tuberculosis, Mixture, lung, Categorical, np.array([0.6, 0.4]), np.array([0.18, 0.82]), np.array([0.11, 0.89]), np.array([0.04, 0.96]))\n",
    "\n",
    "# Observations\n",
    "tuberculosis.observe(TRUE)\n",
    "smoking.observe(FALSE)\n",
    "bronchitis.observe(TRUE)\n",
    "\n",
    "# Perform variational Bayesian inference\n",
    "Q = VB(dyspnea, xray, bronchitis, lung, smoking, tuberculosis, asia)\n",
    "Q.update(repeat=100)\n",
    "\n",
    "# Print the probabilities\n",
    "print(\"P(asia):\", asia.get_moments()[0][TRUE])\n",
    "print(\"P(tuberculosis):\", tuberculosis.get_moments()[0][TRUE])\n",
    "print(\"P(smoking):\", smoking.get_moments()[0][TRUE])\n",
    "print(\"P(lung):\", lung.get_moments()[0][TRUE])\n",
    "print(\"P(bronchitis):\", bronchitis.get_moments()[0][TRUE])\n",
    "print(\"P(xray):\", xray.get_moments()[0][TRUE])\n",
    "print(\"P(dyspnea):\", dyspnea.get_moments()[0][TRUE])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e8aba",
   "metadata": {},
   "source": [
    "# 9)Demonstrate the working of EM algorithm to cluster a set of data stored in a .CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f994572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels:\n",
      "[1 1 1 1 0 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HI\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "X = data.values\n",
    "\n",
    "# Specify the number of clusters\n",
    "num_clusters = 3\n",
    "\n",
    "# Create an instance of Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=num_clusters)\n",
    "\n",
    "# Fit the GMM to the data\n",
    "gmm.fit(X)\n",
    "\n",
    "# Predict the cluster labels for the data points\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "# Print the cluster labels\n",
    "print('Cluster Labels:')\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c00a4",
   "metadata": {},
   "source": [
    "# 10)Demonstrate the working of SVM classifier for a suitable data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6ae26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create an SVM classifier\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "\n",
    "# train the classifier\n",
    "svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d8eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94        43\n",
      "           1       0.95      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# calculate the accuracy and print the classification report of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
